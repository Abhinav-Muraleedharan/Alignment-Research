Alignment Research:


General Directions:

More generally, we are trying to 'understand' an unsafe/unligned model by taking a taylor approximation
around safe model. Now 'understand' is a sloppy word, so let's define the research questions more precisely:

1. Can we compute directions in the activation space that correspond to unsafe behaviours ?
2. Can we compute a subspace in the activation space that would correspond to safe/good behaviours? 
3. Many techniques exist for computing meaningful directions in the activation space. What is the 
advantage of our method as opposed to existing appraoches like applying a linear probe etc ? Taking a taylor 
approximation and arrguing from that is theoretically more rigorous, but can we apply the insights from theory
to improve the model in  a significant way ? 




